{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv   \n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch.nn as nn \n",
    "from torch.nn.functional import leaky_relu, softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "from GANutils import *\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('Data/TU_onehot')\n",
    "data = back_from_dummies(data)\n",
    "data = data.drop(['HomeAdrMunCode'], axis=1)\n",
    "data = encode_onehot(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE()\n",
    "    def __init__(self, train, validation, numerical_col_n, categorical_col_n, categories_n, categories_cum, eval_set, # Data\n",
    "                     col_names, original_df, pre_one_hot_df, # These are important to come back from the VAE samples to the original dataset\n",
    "                     intermediate_dim=256, latent_dim=10, n_hidden_layers=4, drop_rate=0.,# Architecture\n",
    "                     cat_loss_weight=0.5, beta=0.5, # Loss\n",
    "                     batch_size=100, epochs=50, learn_rate=0.001): # Training \n",
    "        \n",
    "        # Data parameters\n",
    "        self.data_train = train\n",
    "        self.data_validation = validation\n",
    "        \n",
    "        self.numerical_col_n = numerical_col_n # Scalar\n",
    "        self.categorical_col_n = categorical_col_n # Scalar\n",
    "        self.categories_n = categories_n # List of scalars\n",
    "        self.categories_cum = categories_cum # List of scalars\n",
    "        self.eval_set = eval_set # Set of variables with which the model will be evaluated\n",
    "        \n",
    "        self.col_names = col_names # column names of the one hot encoded dataset\n",
    "        self.original_df = original_df # original data set, to retrieve its structure\n",
    "        self.pre_one_hot_df = pre_one_hot_df # one hot encoded dataset, to retrieve its structure\n",
    "        \n",
    "        # Architecture parameters\n",
    "        self.input_dim = train.shape[1]\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.latent_dim = latent_dim\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        # Loss parameters\n",
    "        self.cat_loss_weight = cat_loss_weight\n",
    "        self.beta = beta\n",
    "        \n",
    "        # Training parameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learn_rate = learn_rate\n",
    "        \n",
    "        # Sampling parameters\n",
    "        self.n_samples = validation.shape[0]\n",
    "        \n",
    "        # Model variable\n",
    "        self.model = self.create_vae()\n",
    "        \n",
    "        # Session variables\n",
    "        vae_sess = tf.InteractiveSession() # Start tf session so we can run code.\n",
    "        K.set_session(vae_sess) # Connect keras to the created session.     \n",
    "\n",
    "    # Encoder architecture\n",
    "    def create_encoder(self):\n",
    "        '''\n",
    "        This is the encoder architecture, \n",
    "\n",
    "        params: \n",
    "        n_hidden_layers: number of hidden layers\n",
    "        intermediate_dim: value of the number of neurons for the first intermediate layer. They decrease Harmonically (N/2). \n",
    "        latent_dim: dimension of latent space (Try this to be smaller than the number of neurons on the last hidden layer)\n",
    "        activation: activation function.\n",
    "        '''\n",
    "        e_input = Input(shape=(self.input_dim,), name='e_input')\n",
    "        \n",
    "        # Dimension Check\n",
    "        #if self.latent_dim > self.intermediate_dim/(2**(self.n_hidden_layers-1)):\n",
    "        #    print('Choose a smaller latent dimension or a greater intermediate neuron dimension')\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(self.n_hidden_layers):\n",
    "            if _==0: # The first one takes the inputs as input\n",
    "                intermediate = Dense(self.intermediate_dim, name= 'encoder_hidden_{}'.format(_), kernel_initializer='he_uniform')(e_input)\n",
    "                intermediate = BatchNormalization()(intermediate)\n",
    "                intermediate = Activation('relu')(intermediate)\n",
    "                intermediate = Dropout(rate=self.drop_rate)(intermediate)\n",
    "            else: # After the first one, the network takes the intermediate layers as input\n",
    "                intermediate = Dense(self.intermediate_dim, name= 'encoder_hidden_{}'.format(_), kernel_initializer='he_uniform')(intermediate)\n",
    "                intermediate = BatchNormalization()(intermediate)\n",
    "                intermediate = Activation('relu')(intermediate)\n",
    "                intermediate = Dropout(rate=self.drop_rate)(intermediate)\n",
    "            self.intermediate_dim = int(self.intermediate_dim/2) # Update the value of the number of neurons\n",
    "\n",
    "        # Latent space    \n",
    "        e_output = Dense(2*self.latent_dim, name='e_output')(intermediate)\n",
    "\n",
    "        return Model(inputs=e_input, outputs=e_output)\n",
    "\n",
    "    # Sampler and reparametrization trick\n",
    "    def sampling(self, args): #args\n",
    "        \"\"\"Returns sample from a distribution N(args[0], diag(args[1]))\n",
    "\n",
    "        The sample should be computed with reparametrization trick.\n",
    "\n",
    "        The inputs are tf.Tensor\n",
    "            args[0]: (batch_size x latent_dim) mean of the desired distribution\n",
    "            args[1]: (batch_size x latent_dim) logarithm of the variance vector of the desired distribution\n",
    "\n",
    "        Returns:\n",
    "            A tf.Tensor of size (batch_size x latent_dim), the samples.\n",
    "        \"\"\"\n",
    "        # Sampling from the distribution \n",
    "        # q(t | x) = N(t_mean, exp(t_log_var))\n",
    "        # with reparametrization trick.\n",
    "        \n",
    "        z_mean, z_log_var = args \n",
    "        # I don't use the selfs here because I don't know the response, \n",
    "        # should try in any case, though\n",
    "            \n",
    "        samples = tf.random_normal(z_mean.get_shape())\n",
    "        samples = samples * tf.exp(0.5 * z_log_var) + z_mean\n",
    "        return samples\n",
    "\n",
    "    # Decoder architecture\n",
    "    def create_decoder(self):\n",
    "        '''\n",
    "        This is the decoder architecture, \n",
    "\n",
    "        params: \n",
    "        n_hidden_layers: number of hidden layers\n",
    "        intermediate_dim: value of the number of neurons for the first intermediate layer. They decrease Harmonically (N/2). \n",
    "        latent_dim: dimension of latent space (Try this to be smaller than the number of neurons on the last hidden layer)\n",
    "        activation: activation function.\n",
    "        '''\n",
    "\n",
    "        # This returns a tensor\n",
    "        d_input = Input(shape=(self.latent_dim,), name='d_input')\n",
    "\n",
    "        self.intermediate_dim *= 2 # So the number of layers is the same as in the encoder\n",
    "        \n",
    "        # Intermediate layers\n",
    "        for _ in range(self.n_hidden_layers):\n",
    "            if _==0: # The first one takes the inputs as input\n",
    "                intermediate = Dense(self.intermediate_dim, name= 'encoder_hidden_{}'.format(_), kernel_initializer='he_uniform')(d_input)\n",
    "                intermediate = BatchNormalization()(intermediate)\n",
    "                intermediate = Activation('relu')(intermediate)\n",
    "                intermediate = Dropout(rate=self.drop_rate)(intermediate)\n",
    "            else: # After the first one, the network takes the intermediate layers as input\n",
    "                intermediate = Dense(self.intermediate_dim, name= 'encoder_hidden_{}'.format(_), kernel_initializer='he_uniform')(intermediate)\n",
    "                intermediate = BatchNormalization()(intermediate)\n",
    "                intermediate = Activation('relu')(intermediate)\n",
    "                intermediate = Dropout(rate=self.drop_rate)(intermediate)\n",
    "            self.intermediate_dim *= 2 # Update the value of the number of neurons\n",
    "\n",
    "        # Final layer\n",
    "        # Categorical decode\n",
    "        x_decoded_mean_cat = [Dense(self.categories_n[cat], activation='softmax')(intermediate) \n",
    "                              for cat in range(len(self.categories_n))]\n",
    "\n",
    "        if self.numerical_col_n > 0: # If there are numerical variables, concatenate both\n",
    "            x_decoded_mean_num = Dense(self.numerical_col_n)(intermediate) # Numerical decode\n",
    "            d_output = concatenate([x_decoded_mean_num] + x_decoded_mean_cat, name='d_output')\n",
    "        else: # If there are no numerical variables only include the categorical output layer\n",
    "            d_output = concatenate(x_decoded_mean_cat, name='d_output')\n",
    "\n",
    "        return Model(inputs=d_input, outputs=d_output)\n",
    "\n",
    "    # VAE architecture\n",
    "    def create_vae(self):        \n",
    "        # Input of the encoder, encoder creation and input encoding\n",
    "        self.vae_input = Input(batch_shape=(self.batch_size, self.input_dim), name='enc_input') \n",
    "        self.encoder = self.create_encoder()\n",
    "        enc = self.encoder(self.vae_input)\n",
    "        \n",
    "        # Latent space functions\n",
    "        get_z_mean = Lambda(lambda e: e[:, :self.latent_dim])\n",
    "        get_z_log_var = Lambda(lambda e: e[:, self.latent_dim:])\n",
    "\n",
    "        # Normal parameters extraction\n",
    "        z_mean = get_z_mean(enc)\n",
    "        z_log_var = get_z_log_var(enc)\n",
    "        \n",
    "        # Sampling and saving the parameters for loss estimation\n",
    "        self.z = Lambda(self.sampling, name='Sampling')([z_mean, z_log_var])\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "        \n",
    "        # Decoder creation and latent space decoding\n",
    "        self.decoder = self.create_decoder()\n",
    "        self.vae_output = self.decoder(self.z)\n",
    "        \n",
    "        return Model(self.vae_input, self.vae_output)\n",
    "\n",
    "    # Loss function\n",
    "    def vae_loss(self):\n",
    "\n",
    "        # Numerical variables loss\n",
    "        if self.numerical_col_n > 0:\n",
    "            recon_num = mean_squared_error(self.vae_input[:, :self.numerical_col_n], self.vae_output[:, :self.numerical_col_n])\n",
    "        \n",
    "        # Categorical variables loss\n",
    "        recon_cat = 0\n",
    "        for idx in range(len(self.categories_cum)-1):\n",
    "            idx_i = self.numerical_col_n+self.categories_cum[idx] # Initial index\n",
    "            idx_f = self.numerical_col_n+self.categories_cum[idx+1] # Final index\n",
    "            recon_cat += categorical_crossentropy(self.vae_input[:, idx_i:idx_f], self.vae_output[:, idx_i:idx_f])\n",
    "\n",
    "        # Kuellback-Liebler Divergence between a normal and a standard normal\n",
    "        kl = - 0.5 * K.sum(1. + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var))\n",
    "\n",
    "        # Total loss for the numerical case\n",
    "        if self.numerical_col_n > 0:\n",
    "            loss = K.mean(recon_num + self.cat_loss_weight*recon_cat + self.beta*kl)\n",
    "        else:\n",
    "            loss = K.mean(recon_cat  + self.beta*kl) #loss = K.mean(self.cat_loss_weight*recon_cat  + self.beta*kl)        \n",
    "\n",
    "        return loss \n",
    "\n",
    "    # Fit the model\n",
    "    def vae_fit(self): \n",
    "        ########## CALL BACKS\n",
    "        # Define a learning rate schedule\n",
    "        def scheduler(epoch):\n",
    "            if epoch == 100:\n",
    "                K.set_value(self.opt.lr, self.learn_rate/1000)\n",
    "            return K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        change_lr = LearningRateScheduler(scheduler)\n",
    "        \n",
    "        # Define a Plateau Learning Rate changer\n",
    "        plateau_lr = ReduceLROnPlateau()\n",
    "        \n",
    "        # Define terminate on NAN\n",
    "        noNaN = TerminateOnNaN()\n",
    "        \n",
    "        # Define early stopping\n",
    "        early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto') #patience=2\n",
    "        \n",
    "        # Define tensorboard. To use it write on terminal: tensorboard --logdir path_to_current_dir/Graph, enter the tensorboard in browser with: htttp://localhost:6006\n",
    "        tensorboard = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "        \n",
    "        # Train the model\n",
    "        self.opt = keras.optimizers.RMSprop(lr=self.learn_rate) # Optimizer\n",
    "        self.loss = self.vae_loss() # Loss\n",
    "        self.model.compile(optimizer=self.opt, loss=lambda x, y: self.loss)\n",
    "        self.history = self.model.fit(x=self.data_train, shuffle=True, y=self.data_train,\n",
    "                       epochs=self.epochs, batch_size=self.batch_size, validation_data=(self.data_validation, self.data_validation), # This validation_data is added on apr/15\n",
    "                       verbose=2, callbacks=[early_stop, plateau_lr, noNaN]) # callbacks=[change_lr, early_stop, tensorboard, plateau_lr, noNAN]\n",
    "\n",
    "    # Sampling helper function for evaluation\n",
    "    def sampler(self):\n",
    "        z_sample = np.random.normal(0., 1.0, size=(self.n_samples, self.latent_dim))\n",
    "        prediction = self.decoder.predict(z_sample).transpose()\n",
    "        samples = np.zeros((self.input_dim, self.n_samples))\n",
    "        samples[:self.numerical_col_n,:]=prediction[:self.numerical_col_n,:]\n",
    "        for idx in range(len(self.categories_cum)-1):\n",
    "            idx_i = self.numerical_col_n+self.categories_cum[idx] # Initial index\n",
    "            idx_f = self.numerical_col_n+self.categories_cum[idx+1] # Final index\n",
    "            mask = np.argmax(prediction[idx_i:idx_f, :], axis=0) + idx_i\n",
    "            samples[mask, np.arange(len(mask))] = 1\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    # VAE evaluation\n",
    "    def vae_evaluate(self, used_metric='MAE'):\n",
    "        # Fit the model\n",
    "        self.vae_fit()\n",
    "        \n",
    "        # Evaluate it\n",
    "        self.samples = self.sampler()\n",
    "        self.vae_df = TUutils.samples_to_df(self.samples, col_names=self.col_names, original_df=self.original_df, pre_one_hot_df=self.pre_one_hot_df)\n",
    "        #self.validation_df = TUutils.samples_to_df(self.data_validation.transpose(), print_duplicates=False)\n",
    "        self.validation_df  = TUutils.samples_to_df(self.data_validation.transpose(), col_names=self.col_names, original_df=self.original_df, pre_one_hot_df=self.pre_one_hot_df)\n",
    "\n",
    "        ##### Count creator\n",
    "        self.vae_df['count'] = 1\n",
    "        self.vae_df = self.vae_df.groupby(self.eval_set, observed=True).count()\n",
    "        self.vae_df /= self.vae_df['count'].sum()\n",
    "\n",
    "        self.validation_df['count'] = 1\n",
    "        self.validation_df = self.validation_df.groupby(self.eval_set, observed=True).count()\n",
    "        self.validation_df /= self.validation_df['count'].sum()\n",
    "\n",
    "        ##### Merge and difference\n",
    "        real_and_sampled = pd.merge(self.validation_df, self.vae_df, suffixes=['_real', '_sampled'], on=self.eval_set, how='outer') # on= all variables\n",
    "        real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "        real_and_sampled['diff'] = real_and_sampled.count_real-real_and_sampled.count_sampled\n",
    "        diff = np.array(real_and_sampled['diff'])\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['MAE']   = np.mean(abs(diff))\n",
    "        metrics['MSE']   = np.mean(diff**2)\n",
    "        metrics['RMSE']  = np.sqrt(np.mean(diff**2))\n",
    "        metrics['SRMSE'] = metrics['RMSE']/real_and_sampled['count_real'].mean()\n",
    "        print('Evaluating with {}'.format(used_metric))\n",
    "        print('Using variables {}'.format(self.eval_set))\n",
    "        print('MAE:{}, MSE:{}, RMSE:{}, SRMSE:{}'.format(metrics['MAE'], metrics['MSE'], metrics['RMSE'], metrics['SRMSE']))\n",
    "        \n",
    "        return metrics[used_metric]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
