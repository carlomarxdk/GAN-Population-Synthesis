{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import csv   \n",
    "import datetime\n",
    "\n",
    "import math\n",
    "import torch.nn as nn \n",
    "from torch.nn.functional import leaky_relu, softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "from GANutils import *\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:3606: FutureWarning: SparseDataFrame is deprecated and will be removed in a future version.\n",
      "Use a regular DataFrame whose columns are SparseArrays instead.\n",
      "\n",
      "See http://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#migrating for more.\n",
      "\n",
      "  result = self._constructor(new_data).__finalize__(self)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('Data/TU_onehot')\n",
    "data = back_from_dummies(data)\n",
    "data = data.drop(['HomeAdrMunCode'], axis=1)\n",
    "data = encode_onehot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 301\n"
     ]
    }
   ],
   "source": [
    "cat = Counter([v.split('_')[0] for v in list(data.columns)])\n",
    "cat_total = sum(cat.values())\n",
    "cat_n = list(cat.values())\n",
    "INPUT_SIZE = 100\n",
    "TARGET_NUM = len(data.columns)\n",
    "print(INPUT_SIZE, TARGET_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise(size:int, batch_size:int):\n",
    "    '''\n",
    "    Generates a 1-d vector of gaussian sampled random values\n",
    "    '''\n",
    "    n = Variable(torch.randn([batch_size,size]), requires_grad=False)\n",
    "    return n\n",
    "\n",
    "def softmax2onehot(x):\n",
    "    x=x.to(device)\n",
    "    max_idx = torch.argmax(x, 1, keepdim=True).to(device)\n",
    "    one_hot = torch.FloatTensor(x.shape).to(device)\n",
    "    one_hot.zero_().to(device)\n",
    "    return one_hot.scatter_(1, max_idx, 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    unif = torch.rand(*shape).to(device)\n",
    "    g = -torch.log(-torch.log(unif + eps))\n",
    "    return g.to(device)\n",
    "\n",
    "def sample_gumbel_softmax(logits, temperature):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "        logits: Tensor of log probs, shape = BS x k\n",
    "        temperature = scalar\n",
    "        \n",
    "        Output: Tensor of values sampled from Gumbel softmax.\n",
    "                These will tend towards a one-hot representation in the limit of temp -> 0\n",
    "                shape = BS x k\n",
    "    \"\"\"\n",
    "    g = sample_gumbel(logits.shape)\n",
    "    h = (g + logits)/temperature.to(device)\n",
    "    h_max = h.max(dim=-1, keepdim=True)[0]\n",
    "    h = h - h_max\n",
    "    cache = torch.exp(h)\n",
    "    y = cache / cache.sum(dim=-1, keepdim=True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator (nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int, \n",
    "                 temperature: float,\n",
    "                 cat: Counter):\n",
    "        super(Generator, self).__init__()\n",
    "        self.cat = cat\n",
    "        self.cat_n = list(cat.values())\n",
    "        self.output_size = sum(self.cat.values())\n",
    "        self.temperature = torch.Tensor([temperature]).to(device)\n",
    "        self.l1 = nn.Sequential( \n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.BatchNorm1d(hidden_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.l2 = nn.Sequential( \n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.LeakyReLU(negative_slope = 0.2),\n",
    "            nn.BatchNorm1d(hidden_size * 2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.l3 = nn.Sequential( \n",
    "            nn.Linear(hidden_size * 2, hidden_size * 3),\n",
    "            nn.LeakyReLU(negative_slope = 0.2),\n",
    "            nn.BatchNorm1d(hidden_size * 3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.l4 = nn.Sequential( \n",
    "            nn.Linear(hidden_size * 3, hidden_size * 2),\n",
    "            nn.LeakyReLU(negative_slope = 0.2),\n",
    "            nn.BatchNorm1d(hidden_size * 2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out = nn.Sequential( \n",
    "            nn.Linear(hidden_size * 2, self.output_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.l1(x)\n",
    "        x=self.l2(x)\n",
    "        x=self.l3(x)\n",
    "        x=self.l4(x)\n",
    "        x=self.out(x)\n",
    "        ### Softmax per class\n",
    "        x = (x.split(self.cat_n, dim=1))\n",
    "        out = torch.cat([sample_gumbel_softmax(v, temperature = self.temperature) for v in x], dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size:int, output_size=1):\n",
    "        '''\n",
    "        input_size: size of the data\n",
    "        output_size: is always 1 \n",
    "        vanila: if True, Sigmoid is going to applied on the last layer\n",
    "        '''\n",
    "        super(Discriminator,self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Sequential( \n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(256, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def clip(self, thr):\n",
    "        for p in self.parameters():\n",
    "            p.data.clamp_(-thr, thr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TP_WGAN():\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 cat, \n",
    "                 epochs = 5000,\n",
    "                 batch_size=64,           \n",
    "                 gen_learn_rate=1E-4, # 4   \n",
    "                 disc_learn_rate=1E-5, # 5\n",
    "                 gamma = 10, \n",
    "                 temperature = 1E-3): \n",
    "        #Data\n",
    "        self.cat = cat\n",
    "        self.cat_n = list(cat.values())\n",
    "        self.onehot_size = sum(self.cat.values())\n",
    "        self.train_val_split(data, batch_size)\n",
    "        \n",
    "        #Networks\n",
    "        self.G = Generator(input_size=INPUT_SIZE, hidden_size=256,  temperature=temperature, cat=self.cat).to(device)\n",
    "        self.D = Discriminator(input_size=TARGET_NUM).to(device)\n",
    "        \n",
    "\n",
    "        \n",
    "        #Parameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.gen_learn_rate = gen_learn_rate\n",
    "        self.disc_learn_rate = disc_learn_rate\n",
    "        self.gamma = gamma\n",
    "        self.temperature = temperature \n",
    "\n",
    "        ''''\n",
    "        ADAM optimizer does not give good results \n",
    "        \n",
    "        self.generator_optim = torch.optim.Adam(self.G.parameters(), gen_learn_rate, betas=(0.5, 0.999))\n",
    "        self.discriminator_optim = torch.optim.Adam(self.D.parameters(), disc_learn_rate, betas=(0.5, 0.999))\n",
    "    \n",
    "        '''\n",
    "        self.generator_optim = torch.optim.RMSprop(self.G.parameters(),\n",
    "                                                  lr = self.gen_learn_rate,\n",
    "                                                  centered=True)\n",
    "        self.discriminator_optim = torch.optim.RMSprop(self.D.parameters(),\n",
    "                                                      lr = self.disc_learn_rate,\n",
    "                                            centered=True)\n",
    "    \n",
    "    def train_val_split(self, data, batch_size):\n",
    "        train, val = train_test_split(data, test_size=0.3)\n",
    "        self.train = DataLoader(torch.tensor(train.values), \n",
    "                                                batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        self.val  = DataLoader(torch.tensor(val.values), \n",
    "                                                batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    def sample(self, n_samples: int):\n",
    "        '''\n",
    "        Generate the data data with Generator network\n",
    "        n_samples: usually equals to the batch size\n",
    "        '''\n",
    "        z = gen_noise(INPUT_SIZE, n_samples)\n",
    "        z = Variable(z, requires_grad=False).to(device)\n",
    "        return self.G.forward(z)\n",
    "\n",
    "    def reset_gradient(self):\n",
    "        self.D.zero_grad()\n",
    "        self.G.zero_grad()\n",
    "        \n",
    "    def grad_penalty(self, data, generated_data):\n",
    "        batch_size = data.size(0)\n",
    "        epsilon = torch.rand(batch_size, TARGET_NUM)\n",
    "        epsilon = epsilon.expand_as(data)\n",
    "        epsilon = epsilon.to(device)\n",
    "\n",
    "        interpolation = epsilon * data + (1 - epsilon) * generated_data\n",
    "        interpolation = Variable(interpolation, requires_grad=True)\n",
    "        interpolation = interpolation.to(device)\n",
    "\n",
    "        interpolation_logits = self.D(interpolation)\n",
    "        grad_outputs = torch.ones(interpolation_logits.size()).to(device)\n",
    "\n",
    "\n",
    "        gradients = torch.autograd.grad(outputs=interpolation_logits,\n",
    "                                  inputs=interpolation,\n",
    "                                  grad_outputs=grad_outputs,\n",
    "                                  create_graph=True,\n",
    "                                  retain_graph=True)[0]\n",
    "\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "        return self.gamma * ((gradients_norm - 1) ** 2).mean()\n",
    "        \n",
    "        \n",
    "    def fit(self, n_critic=1, n_gen=3, to_log=True):\n",
    "        filename = 'Logs/wgangp-{date:%Y-%m-%d_%H:%M:%S}'.format( date=datetime.datetime.now() )\n",
    "        filename = filename.replace(':', '')\n",
    "        self.discriminator_loss, self.generator_loss = [], []\n",
    "        for epoch in range(self.epochs):\n",
    "            gen_gradient = 0\n",
    "            batch_d_loss, batch_g_loss = [], []\n",
    "            batch_gp , batch_rs , batch_fs = [], [], []\n",
    "            for x in self.val:\n",
    "                ## Reset gradient for both networks (on new epoch)\n",
    "                self.reset_gradient()\n",
    "                a = list(self.G.parameters())[0].clone()\n",
    "                a1 = list(self.G.parameters())[1].clone()\n",
    "                a2 = list(self.G.parameters())[2].clone()\n",
    "                a3 = list(self.G.parameters())[3].clone()\n",
    "                a4 = list(self.G.parameters())[4].clone()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                x = Variable(x).float().to(device)\n",
    "                ## Determine the batch size\n",
    "                batch_size = x.shape[0]\n",
    "                #STEP 1. TRAIN THE GENERATOR (if n_gen is larger than 1)\n",
    "                if (n_gen-1)>0:\n",
    "                    for _ in range(n_gen-1):\n",
    "                        x_fake = self.sample(batch_size).to(device)\n",
    "                        \n",
    "                        output = self.D.forward(x_fake)\n",
    "                        G_loss = -torch.mean(output)\n",
    "                        G_loss.backward()\n",
    "                        \n",
    "                        self.generator_optim.step()\n",
    "                        self.reset_gradient()\n",
    "                        batch_g_loss.append(G_loss.item())\n",
    "\n",
    "                        #for param in self.G.parameters():\n",
    "                        #        print(param.grad.data.sum())\n",
    "\n",
    "                        # start debugger\n",
    "                        #import pdb; pdb.set_trace()\n",
    "\n",
    "\n",
    "                # STEP 2. TRAIN THE DISCRIMINATOR (With gradient penalty)\n",
    "                if n_critic <= 0: n_critic=1\n",
    "                for _ in range(n_critic):\n",
    "                    \n",
    "                    output_true = self.D.forward(x)\n",
    "                    # Step 2.1 Generate fake data G(z), where z ~ N(0, 1)\n",
    "                    #         is a latent code.\n",
    "                    x_fake = self.sample(batch_size).to(device)\n",
    "                    # Step 3. Send fake data through discriminator\n",
    "                    #         propagate error and update D weights.\n",
    "                    # --------------------------------------------\n",
    "                    # Note: detach() is used to avoid compounding generator gradients\n",
    "                    output_fake = self.D.forward(x_fake.detach()) \n",
    "                    \n",
    "                    gp = self.grad_penalty(x, x_fake)\n",
    "                    D_loss = -(torch.mean(output_true) - torch.mean(output_fake)) + gp\n",
    "                    D_loss.backward()\n",
    "                    self.discriminator_optim.step()\n",
    "                    \n",
    "                    #Reset the gradient\n",
    "                    self.reset_gradient()\n",
    "                    batch_d_loss.append(D_loss.item())\n",
    "                    batch_gp.append(gp.item())\n",
    "                    batch_rs.append(torch.mean(output_true).item())\n",
    "                    batch_fs.append(torch.mean(output_fake).item())\n",
    "                \n",
    "                # Step 4. Send fake data through discriminator _again_\n",
    "                #         propagate the error of the generator and\n",
    "                #         update G weights.\n",
    "                #x_fake = self.sample(batch_size).to(device)\n",
    "                #x_fake = (x_fake.split(self.cat_n, dim=1))\n",
    "                #x_fake = torch.cat([sample_gumbel_softmax(v, self.temperature) for v in x_fake], dim=1)\n",
    "                output = self.D.forward(x_fake)\n",
    "                G_loss = -torch.mean(output)\n",
    "                G_loss.backward()\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    for param in self.G.parameters():\n",
    "                        gen_gradient += param.grad.data.sum()\n",
    "                except:\n",
    "                    print('Unstable generator')\n",
    "                self.generator_optim.step()\n",
    "                b = list(self.G.parameters())[0].clone()\n",
    "                b1 = list(self.G.parameters())[1].clone()\n",
    "                b2 = list(self.G.parameters())[2].clone()\n",
    "                b3 = list(self.G.parameters())[3].clone()\n",
    "                b4 = list(self.G.parameters())[4].clone()        \n",
    "                batch_fs.append(torch.mean(output_fake).item())\n",
    "                batch_g_loss.append(G_loss.item())\n",
    "            \n",
    "            self.discriminator_loss.append(np.mean(batch_d_loss))\n",
    "            self.generator_loss.append(np.mean(batch_g_loss))\n",
    "            clear_output()\n",
    "\n",
    "            print(\"Generator gradient: %.7f\" %gen_gradient, 'Weight Update %s %s %s %s %s' % (torch.equal(a.data, b.data),\n",
    "                                                                                              torch.equal(a1.data, b1.data),\n",
    "                                                                                              torch.equal(a2.data, b2.data),\n",
    "                                                                                              torch.equal(a3.data, b3.data),\n",
    "                                                                                              torch.equal(a4.data, b4.data)\n",
    "                                                                                             ))            \n",
    "            #### Output per epoch\n",
    "     \n",
    "            print(\"Epoch: %3d || D Loss: %5.5f (rs:%3.3f fs:%3.3f gp:%3.3f) || G Loss: %5.5f \" %(epoch, \n",
    "                                                                                                 np.mean(batch_d_loss), \n",
    "                                                                                                 np.mean(batch_rs),\n",
    "                                                                                                 np.mean(batch_fs),\n",
    "                                                                                                 np.mean(batch_gp),\n",
    "                                                                                                 np.mean(batch_g_loss)))\n",
    "\n",
    "            # -- Plotting --\n",
    "            f, axarr = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "            # Loss\n",
    "            axarr[0].set_xlabel('Epoch')\n",
    "            axarr[0].set_ylabel('Loss')\n",
    "            axarr[0].set_title('Discriminator Loss || lr= %s' %self.disc_learn_rate )\n",
    "            axarr[1].set_xlabel('Epoch')\n",
    "            axarr[1].set_ylabel('Loss')\n",
    "            axarr[1].set_title('Generator Loss  || lr= %s' %self.gen_learn_rate )\n",
    "\n",
    "            axarr[0].plot(np.arange(epoch+1), self.discriminator_loss)\n",
    "            axarr[1].plot(np.arange(epoch+1), self.generator_loss, linestyle=\"--\")\n",
    "            plt.show()\n",
    "            if to_log:\n",
    "                self.log(filename, epoch, np.mean(batch_d_loss), np.mean(batch_g_loss), np.mean(batch_rs),\n",
    "                    np.mean(batch_fs),  np.mean(batch_gp))\n",
    "            #print(x_fake[0])\n",
    "            #print(x[0])\n",
    "\n",
    "    def synthesise(self, num=2):\n",
    "        data_dummy = pd.DataFrame(columns=data.columns, dtype=np.int32)\n",
    "        x_fake = self.sample(num)\n",
    "        x_fake = x_fake.split(self.cat_n, dim=1)\n",
    "        x_fake = torch.cat([softmax2onehot(v) for v in x_fake], dim=1)\n",
    "        x_fake = np.array(x_fake.cpu())[0].astype(int)\n",
    "        data_dummy.loc[-1] =np.array(x_fake)[0].astype(int)\n",
    "        return back_from_dummies(data_dummy)\n",
    "    def log(self, name, epoch, d_loss, g_loss, rs, fs, gp):\n",
    "        fields=[epoch, d_loss, g_loss, rs, fs, gp]\n",
    "        with open(r''+name + '.csv', 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(fields)\n",
    "        if epoch % 50 == 0:\n",
    "            torch.save(self.G, name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = TP_WGAN(data = data, cat = cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.fit(n_critic=3, n_gen=1) ##add num of critics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## batch size change (was the most influential part) + lr increase for generator ## remove category with too much labels\n",
    "### 4 6 - got a bit down after epoch 7000\n",
    "### clip the rresult\n",
    "\n",
    "##batch normalization really helped - discriminator is tricked by gen quite fast \n",
    "### low temperature did not allow to gradient to flow throught "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
